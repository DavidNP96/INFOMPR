{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1cTU454Bkr7",
        "outputId": "d66e6edf-92a7-4210-d9f9-21b5653bdd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.15.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Number of extracted sequences: 2312\n",
            "Epoch 1/6\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7437 - accuracy: 0.1354\n",
            "Epoch 00001: loss improved from inf to 3.74371, saving model to model_weights/baseline-ton-01-3.7437.hdf5\n",
            "10/10 [==============================] - 10s 628ms/step - loss: 3.7437 - accuracy: 0.1354\n",
            "Epoch 2/6\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4557 - accuracy: 0.0748\n",
            "Epoch 00002: loss improved from 3.74371 to 3.45574, saving model to model_weights/baseline-ton-02-3.4557.hdf5\n",
            "10/10 [==============================] - 6s 621ms/step - loss: 3.4557 - accuracy: 0.0748\n",
            "Epoch 3/6\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1554 - accuracy: 0.1648\n",
            "Epoch 00003: loss improved from 3.45574 to 3.15544, saving model to model_weights/baseline-ton-03-3.1554.hdf5\n",
            "10/10 [==============================] - 6s 618ms/step - loss: 3.1554 - accuracy: 0.1648\n",
            "Epoch 4/6\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0910 - accuracy: 0.1817\n",
            "Epoch 00004: loss improved from 3.15544 to 3.09098, saving model to model_weights/baseline-ton-04-3.0910.hdf5\n",
            "10/10 [==============================] - 6s 615ms/step - loss: 3.0910 - accuracy: 0.1817\n",
            "Epoch 5/6\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0960 - accuracy: 0.1808\n",
            "Epoch 00005: loss did not improve from 3.09098\n",
            "10/10 [==============================] - 6s 607ms/step - loss: 3.0960 - accuracy: 0.1808\n",
            "Epoch 6/6\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0841 - accuracy: 0.1812\n",
            "Epoch 00006: loss improved from 3.09098 to 3.08409, saving model to model_weights/baseline-ton-06-3.0841.hdf5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: 3.0841 - accuracy: 0.1812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM_trained/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM_trained/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f1b6be1a350> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f1b61b34b50> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.3.5\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RNN\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pickle\n",
        "import os\n",
        "from numpy.random import choice\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open(f\"/content/drive/My Drive/Colab Notebooks/cpt_2_data.p\", \"rb\") as f2:\n",
        "    data = pickle.load(f2)\n",
        "\n",
        "DATASIZE = 10\n",
        "\n",
        "data = data.loc[0:DATASIZE-1]['cpt_input']\n",
        "\n",
        "text = \" \".join(data.to_numpy())\n",
        "\n",
        "characters = sorted(list(set(\" \".join(text)+'`')))\n",
        "vocab_size = len(characters)\n",
        "\n",
        "X = []   # extracted sequences\n",
        "Y = []   # the target - the follow up character\n",
        "\n",
        "seq_length = 180   #number of characters to consider before predicting the following character\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "\n",
        "length = len(text)\n",
        "\n",
        "for i in range(0, length - seq_length, 1):\n",
        "    sequence = text[i:i + seq_length]\n",
        "    label = text[i + seq_length]\n",
        "    X.append([char_to_n[char] for char in sequence])\n",
        "    Y.append(char_to_n[label])\n",
        "    \n",
        "print('Number of extracted sequences:', len(X))\n",
        "    \n",
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "X_modified = X_modified / float(len(characters))\n",
        "Y_modified = np_utils.to_categorical(Y)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(600, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(600))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "filepath=\"model_weights/baseline-ton-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X_modified, Y_modified, epochs=6, batch_size=256, callbacks = callbacks_list)\n",
        "\n",
        "model.save(\"LSTM_trained\")\n"
      ]
    }
  ]
}